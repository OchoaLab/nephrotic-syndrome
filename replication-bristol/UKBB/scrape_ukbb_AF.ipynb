{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96478203-06c8-4c0f-b2fb-564f3dc33222",
   "metadata": {},
   "source": [
    "Dynamic website scraping of UKBB AF browser: https://afb.ukbiobank.ac.uk/\n",
    "* extracts allele count and total allele N table for each ancestry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105b29b5-e3e0-4707-a128-2b46bb68df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a3b82-6ac0-4cc5-8b3d-9743b4daeda9",
   "metadata": {},
   "source": [
    "* Create text files with UKBB URLs of interest, read URL text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10067283-73bf-49b1-a72e-e42d6b62149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ssns_ctrl_url.txt', 'r') as file:\n",
    "    # Read the lines of the file into a list\n",
    "    urls = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "878384bb-1dff-4b8d-abef-7c5bc843baee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a339135b-7bd3-48bd-8b6a-7a1a795a5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "timeout_urls = []\n",
    "driver = webdriver.Chrome()\n",
    "# Iterate over each URL\n",
    "for url in urls:\n",
    "    # Send a GET request to the URL\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        # If no error message, wait for page to load, extract variant\n",
    "        variant_element = WebDriverWait(driver, 8).until(\n",
    "            EC.visibility_of_element_located((By.CSS_SELECTOR, '.MuiTypography-root.MuiTypography-h5.css-zq6grw'))\n",
    "        )\n",
    "        variant = variant_element.text.strip()\n",
    "        #print(variant)\n",
    "    except TimeoutException:\n",
    "        #print(f\"Could not fetch data for URL: {url} within the specified time\")\n",
    "        # if the 'variant' is not extracted, meaning page wasn't loaded/got an error, save the URL into a list\n",
    "        timeout_urls.append(url)\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    # Find all population divs\n",
    "    population_divs = soup.find_all('div', class_='ag-row')\n",
    "    # Create an empty list to store AC/AN table for population\n",
    "    data = []\n",
    "    \n",
    "    # Iterate over each population div\n",
    "    for population_div in population_divs:\n",
    "        # Find population name, allele count, and allele number within each population div\n",
    "        population_name_element = population_div.find('div', attrs={'col-id': 'population'})\n",
    "        allele_count_element = population_div.find('div', attrs={'col-id': 'alleleCount'})\n",
    "        allele_num_element = population_div.find('div', attrs={'col-id': 'alleleNum'})\n",
    "        \n",
    "        # Check if all elements were found\n",
    "        if population_name_element and allele_count_element and allele_num_element:\n",
    "            population_name = population_name_element.text.strip()\n",
    "            allele_count = allele_count_element.text.strip()\n",
    "            allele_num = allele_num_element.text.strip()\n",
    "            \n",
    "            # Append population data to the list\n",
    "            data.append([population_name, allele_count, allele_num])\n",
    "\n",
    "    # save AC/AN for each population in a dataframe\n",
    "    df = pd.DataFrame(data, columns=['Population', 'Allele Count', 'Allele Num'])\n",
    "    \n",
    "    # Add a new column 'Variant' with the extracted variant value, so we know which SNP we are scraping\n",
    "    df['Variant'] = variant\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a6d2202-a9c1-4911-b8ca-ea2f3f310651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all variants extracted together\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "final_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26ae5ce2-c65c-4b96-a491-69329d5c4835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save extracted dataframe and also timeout URLs\n",
    "final_df.to_csv('final_dataframe_ssns_6.csv', index=False)\n",
    "file_path = \"timeout_urls_ssns_6.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, \"w\") as file:\n",
    "    # Iterate over each element in the list\n",
    "    for element in timeout_urls:\n",
    "        # Write the element to the file followed by a newline character\n",
    "        file.write(element + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
